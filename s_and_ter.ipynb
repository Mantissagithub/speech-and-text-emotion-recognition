{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube pydub transformers torchaudio librosa moviepy openai-whisper yt_dlp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh20q-ElMNe3",
        "outputId": "730d555d-7a83-4abf-85bb-83aeafada882"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231117)\n",
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2024.5.27-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.3.0+cu121)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchaudio) (12.5.40)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.7.0)\n",
            "Collecting brotli (from yt_dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2024.6.2)\n",
            "Collecting mutagen (from yt_dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt_dlp)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2.0.7)\n",
            "Collecting websockets>=12.0 (from yt_dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchaudio) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchaudio) (1.3.0)\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt_dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-12.0 yt_dlp-2024.5.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "import torch\n",
        "import yt_dlp\n",
        "\n",
        "# URL of the YouTube Shorts video\n",
        "VIDEO_URL = 'https://youtube.com/shorts/4tBQV0tqc3I?si=w0SHBYZKksc8Bl-p'  # Replace with actual URL\n",
        "\n",
        "# Download the video using yt-dlp\n",
        "ydl_opts = {}\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([VIDEO_URL])\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Load the audio file\n",
        "audio_path = 'audio.mp4'\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(audio_path)\n",
        "transcript = result['text']\n",
        "\n",
        "print(transcript)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqnYoQi2MOcz",
        "outputId": "6957c127-ab8e-41d4-a729-267ffe60533b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtube.com/shorts/4tBQV0tqc3I?si=w0SHBYZKksc8Bl-p\n",
            "[youtube] 4tBQV0tqc3I: Downloading webpage\n",
            "[youtube] 4tBQV0tqc3I: Downloading ios player API JSON\n",
            "[youtube] 4tBQV0tqc3I: Downloading player 84314bef\n",
            "[youtube] 4tBQV0tqc3I: Downloading m3u8 information\n",
            "[info] 4tBQV0tqc3I: Downloading 1 format(s): 247+251\n",
            "[download] Destination: kadhal kappal vibe girl💃 ｜｜ #instatrending #shorts [4tBQV0tqc3I].f247.webm\n",
            "[download] 100% of    3.84MiB in 00:00:00 at 10.27MiB/s  \n",
            "[download] Destination: kadhal kappal vibe girl💃 ｜｜ #instatrending #shorts [4tBQV0tqc3I].f251.webm\n",
            "[download] 100% of  294.46KiB in 00:00:00 at 2.86MiB/s   \n",
            "[Merger] Merging formats into \"kadhal kappal vibe girl💃 ｜｜ #instatrending #shorts [4tBQV0tqc3I].webm\"\n",
            "Deleting original file kadhal kappal vibe girl💃 ｜｜ #instatrending #shorts [4tBQV0tqc3I].f251.webm (pass -k to keep)\n",
            "Deleting original file kadhal kappal vibe girl💃 ｜｜ #instatrending #shorts [4tBQV0tqc3I].f247.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This is the fastest way to live in your life. Wake up, take your phone, see who has texted you. Let others decide how you feel about yourself right after you wake up. F**k water, that's where losers. And whenever you see someone put your head down and start scrolling. Who the f**k would have awkward conversation with strangers? Keep telling yourself that you're an introvert and never, I mean never come out of your comfort zone. Got some work to do? Everyone's got it. So f**k it, let's watch them pawn instead. And where are these people running here? And these people are lifting weights, losers. Yeah, I will replace them. Let's not waste time. Let's play some video games. What some videos. Also, this video seems fun. Let's comment and take it again. F**k, f**k.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import moviepy.editor as mp\n",
        "\n",
        "# Convert audio to WAV format\n",
        "audio = AudioSegment.from_file('audio.mp4')\n",
        "audio.export('audio.wav', format='wav')\n",
        "\n",
        "# Alternatively, using moviepy to extract audio\n",
        "# video = mp.VideoFileClip(\"audio.mp4\")\n",
        "# video.audio.write_audiofile(\"audio.wav\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izMa2JqbMrOh",
        "outputId": "2792ad17-4621-46d4-d02e-6339b9671c62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='audio.wav'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
        "import torch\n",
        "import librosa\n",
        "\n",
        "# Load the pre-trained model and feature extractor\n",
        "model_name = \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "# Load the audio file\n",
        "speech_array, sampling_rate = librosa.load('audio.wav', sr=16000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v92_-58dMr9K",
        "outputId": "6de33b21-7221-412c-d0ac-c84321afa12b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at r-f/wav2vec-english-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Assuming `model` and `feature_extractor` are defined properly in your actual code\n",
        "\n",
        "# Preprocess the audio\n",
        "inputs = feature_extractor(speech_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Move model and inputs to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "inputs = {key: inputs[key].to(device) for key in inputs}\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Process the results\n",
        "predicted_ids = torch.argmax(logits, dim=-1).item()\n",
        "emotions = model.config.id2label\n",
        "scores = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "# Output the results\n",
        "speech_emotion_output = [{\"Emotion\": emotions[i], \"Score\": f\"{score * 100:.2f}%\"} for i, score in enumerate(scores)]\n",
        "print(speech_emotion_output)\n",
        "\n",
        "# Find the most predicted emotion\n",
        "most_predicted_index = np.argmax(scores)\n",
        "most_predicted_emotion = emotions[most_predicted_index]\n",
        "print(f\"Most predicted emotion: {most_predicted_emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qauDOuFOMv7V",
        "outputId": "8c9458a9-e8de-4c49-ed15-a4a0aa2fa606"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'Emotion': 'angry', 'Score': '13.38%'}, {'Emotion': 'disgust', 'Score': '13.72%'}, {'Emotion': 'fear', 'Score': '14.21%'}, {'Emotion': 'happy', 'Score': '14.54%'}, {'Emotion': 'neutral', 'Score': '14.76%'}, {'Emotion': 'sad', 'Score': '14.49%'}, {'Emotion': 'surprise', 'Score': '14.91%'}]\n",
            "Most predicted emotion: surprise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "import whisper  # Assuming this is the library you're using for transcription\n",
        "\n",
        "# Function to process text and get emotion vector and predicted emotion\n",
        "def process_text(text, num_labels):\n",
        "    # Load the emo2vec model and tokenizer for 4 or 6 classes based on num_labels\n",
        "    if num_labels == 4:\n",
        "        model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
        "        emotion_labels = ['anger', 'joy', 'optimism', 'sad']\n",
        "    elif num_labels == 6:\n",
        "        model_name = \"bert-base-uncased\"  # Use BERT for 6 labels\n",
        "        emotion_labels = ['anger', 'joy', 'optimism', 'sad', 'fear', 'disgust']\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected number of labels. Please specify either 4 or 6.\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "    emotion_probs = F.softmax(logits, dim=-1).squeeze()\n",
        "    predicted_emotion = emotion_labels[torch.argmax(emotion_probs)]\n",
        "    emotion_dict = {emotion_labels[i]: emotion_probs[i].item() for i in range(len(emotion_labels))}\n",
        "    return emotion_dict, predicted_emotion\n",
        "\n",
        "# Process the transcript for emotions\n",
        "num_labels = 6  # Change this to 4 or 6 as needed\n",
        "emotion_dict, predicted_emotion = process_text(transcript, num_labels)\n",
        "\n",
        "# Print the results\n",
        "print(\"Transcript:\", transcript)\n",
        "print(\"Emotion Probabilities:\", emotion_dict)\n",
        "print(\"Predicted Emotion:\", predicted_emotion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNUzAAc0Qcwd",
        "outputId": "1b0492d0-4370-49c0-9c47-90cbe8c0f2bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript:  This is the fastest way to live in your life. Wake up, take your phone, see who has texted you. Let others decide how you feel about yourself right after you wake up. F**k water, that's where losers. And whenever you see someone put your head down and start scrolling. Who the f**k would have awkward conversation with strangers? Keep telling yourself that you're an introvert and never, I mean never come out of your comfort zone. Got some work to do? Everyone's got it. So f**k it, let's watch them pawn instead. And where are these people running here? And these people are lifting weights, losers. Yeah, I will replace them. Let's not waste time. Let's play some video games. What some videos. Also, this video seems fun. Let's comment and take it again. F**k, f**k.\n",
            "Emotion Probabilities: {'anger': 0.17448261380195618, 'joy': 0.17655262351036072, 'optimism': 0.14043685793876648, 'sad': 0.2100958675146103, 'fear': 0.15233784914016724, 'disgust': 0.1460941731929779}\n",
            "Predicted Emotion: sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# # Speech emotion recognition output\n",
        "# speech_emotion_output = scores\n",
        "\n",
        "# # Text emotion recognition output\n",
        "# text_emotion_output = emotion_dict\n",
        "\n",
        "# Combine the emotion probabilities from speech and text models\n",
        "speech_emotion_probs = np.array([float(d['Score'].replace('%', '')) / 100 for d in speech_emotion_output])\n",
        "text_emotion_probs = np.array(list(emotion_dict.values()))\n",
        "\n",
        "# Ensure the arrays have the same length\n",
        "max_length = max(len(speech_emotion_probs), len(text_emotion_probs))\n",
        "speech_emotion_probs = np.pad(speech_emotion_probs, (0, max_length - len(speech_emotion_probs)), mode='constant', constant_values=0)\n",
        "text_emotion_probs = np.pad(text_emotion_probs, (0, max_length - len(text_emotion_probs)), mode='constant', constant_values=0)\n",
        "\n",
        "# Combine the emotion probabilities\n",
        "combined_emotion_probs = (speech_emotion_probs + text_emotion_probs) / 2\n",
        "\n",
        "# Determine the most predicted emotion\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise', 'anger', 'joy', 'optimism', 'sadness', 'fear', 'disgust']\n",
        "most_predicted_index = np.argmax(combined_emotion_probs)\n",
        "most_predicted_emotion = emotion_labels[most_predicted_index]\n",
        "\n",
        "# Create the combined output emotion vector with labels\n",
        "combined_emotion_vector = [f\"{emotion_labels[i]}:{combined_emotion_probs[i]:.4f}\" for i in range(len(combined_emotion_probs))]\n",
        "\n",
        "\n",
        "print(\"Combined Emotion Vector:\", combined_emotion_vector)\n",
        "print(\"Most predicted emotion:\", most_predicted_emotion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW6SSk7gSPEC",
        "outputId": "1cbcd978-bb43-432f-ec1b-ccdec7639696"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Emotion Vector: ['angry:0.1541', 'disgust:0.1569', 'fear:0.1413', 'happy:0.1777', 'neutral:0.1500', 'sad:0.1455', 'surprise:0.0746']\n",
            "Most predicted emotion: happy\n"
          ]
        }
      ]
    }
  ]
}